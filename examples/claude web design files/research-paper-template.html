<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Multi-Modal Shot Complexity Classification | VFX Intelligence Research</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        :root {
            --primary-blue: #1e3a8a;
            --accent-cyan: #06b6d4;
            --accent-amber: #f59e0b;
            --bg-white: #ffffff;
            --bg-gray: #fafafa;
            --bg-dark: #1f2937;
            --text-dark: #1f2937;
            --text-gray: #6b7280;
            --border-color: #e5e7eb;
            --code-bg: #1e1e1e;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Helvetica Neue', Arial, sans-serif;
            line-height: 1.8;
            color: var(--text-dark);
            background: var(--bg-white);
        }

        /* Simplified Nav */
        nav {
            background: var(--bg-white);
            border-bottom: 1px solid var(--border-color);
            padding: 1rem 2rem;
            position: sticky;
            top: 0;
            z-index: 100;
        }

        .nav-inner {
            max-width: 1200px;
            margin: 0 auto;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        .logo {
            font-weight: 700;
            color: var(--primary-blue);
            text-decoration: none;
        }

        .breadcrumb {
            color: var(--text-gray);
            font-size: 0.875rem;
        }

        .breadcrumb a {
            color: var(--primary-blue);
            text-decoration: none;
        }

        /* Article Header */
        .article-header {
            max-width: 800px;
            margin: 3rem auto 2rem;
            padding: 0 2rem;
        }

        .paper-tag {
            background: var(--primary-blue);
            color: white;
            padding: 0.375rem 1rem;
            border-radius: 4px;
            font-size: 0.75rem;
            font-weight: 700;
            text-transform: uppercase;
            letter-spacing: 0.5px;
            display: inline-block;
            margin-bottom: 1rem;
        }

        h1 {
            font-size: 2.5rem;
            font-weight: 800;
            line-height: 1.2;
            margin-bottom: 1.5rem;
        }

        .author-info {
            display: flex;
            align-items: center;
            gap: 1rem;
            padding: 1rem 0;
            border-top: 1px solid var(--border-color);
            border-bottom: 1px solid var(--border-color);
            margin-bottom: 2rem;
        }

        .author-avatar {
            width: 48px;
            height: 48px;
            border-radius: 50%;
            background: linear-gradient(135deg, var(--primary-blue), var(--accent-cyan));
        }

        .author-details {
            flex: 1;
        }

        .author-name {
            font-weight: 600;
            color: var(--text-dark);
        }

        .author-meta {
            font-size: 0.875rem;
            color: var(--text-gray);
        }

        .paper-actions {
            display: flex;
            gap: 1rem;
            margin-bottom: 2rem;
        }

        .action-btn {
            padding: 0.75rem 1.5rem;
            border-radius: 6px;
            text-decoration: none;
            font-weight: 600;
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
            transition: all 0.2s;
        }

        .btn-primary {
            background: var(--primary-blue);
            color: white;
        }

        .btn-primary:hover {
            background: #1e40af;
        }

        .btn-secondary {
            background: white;
            color: var(--text-dark);
            border: 1px solid var(--border-color);
        }

        .btn-secondary:hover {
            background: var(--bg-gray);
        }

        /* Abstract */
        .abstract {
            background: var(--bg-gray);
            padding: 2rem;
            border-radius: 8px;
            margin-bottom: 3rem;
            border-left: 4px solid var(--primary-blue);
        }

        .abstract h2 {
            font-size: 1.25rem;
            font-weight: 700;
            margin-bottom: 1rem;
            color: var(--primary-blue);
        }

        .abstract p {
            line-height: 1.8;
        }

        /* Table of Contents */
        .toc {
            background: white;
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 2rem;
            margin-bottom: 3rem;
            position: sticky;
            top: 100px;
        }

        .toc h3 {
            font-size: 1rem;
            font-weight: 700;
            margin-bottom: 1rem;
            text-transform: uppercase;
            letter-spacing: 0.5px;
            color: var(--text-gray);
        }

        .toc ul {
            list-style: none;
        }

        .toc li {
            margin-bottom: 0.5rem;
        }

        .toc a {
            color: var(--text-dark);
            text-decoration: none;
            transition: color 0.2s;
        }

        .toc a:hover {
            color: var(--primary-blue);
        }

        /* Main Content Layout */
        .content-wrapper {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 2rem;
            display: grid;
            grid-template-columns: 800px 1fr;
            gap: 3rem;
            align-items: start;
        }

        .main-content {
            max-width: 800px;
        }

        .sidebar {
            position: sticky;
            top: 80px;
        }

        /* Typography */
        .main-content h2 {
            font-size: 1.75rem;
            font-weight: 700;
            margin: 3rem 0 1rem;
            color: var(--text-dark);
        }

        .main-content h3 {
            font-size: 1.375rem;
            font-weight: 600;
            margin: 2rem 0 1rem;
        }

        .main-content p {
            margin-bottom: 1.5rem;
            line-height: 1.8;
        }

        /* Figures */
        .figure {
            margin: 2rem 0;
            background: var(--bg-gray);
            border-radius: 8px;
            padding: 1.5rem;
        }

        .figure-image {
            width: 100%;
            height: 400px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            border-radius: 6px;
            margin-bottom: 1rem;
            display: flex;
            align-items: center;
            justify-content: center;
            color: white;
            font-weight: 600;
        }

        .figure-caption {
            font-size: 0.875rem;
            color: var(--text-gray);
            line-height: 1.6;
        }

        .figure-caption strong {
            color: var(--text-dark);
        }

        /* Code Blocks */
        .code-block {
            background: var(--code-bg);
            color: #d4d4d4;
            padding: 1.5rem;
            border-radius: 8px;
            margin: 2rem 0;
            overflow-x: auto;
            font-family: 'Fira Code', monospace;
            font-size: 0.875rem;
            line-height: 1.6;
        }

        .code-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 1rem;
            padding-bottom: 0.5rem;
            border-bottom: 1px solid #404040;
        }

        .code-lang {
            color: #9ca3af;
            font-size: 0.75rem;
            font-weight: 600;
            text-transform: uppercase;
        }

        .copy-btn {
            background: #374151;
            color: white;
            border: none;
            padding: 0.25rem 0.75rem;
            border-radius: 4px;
            font-size: 0.75rem;
            cursor: pointer;
        }

        /* Results Table */
        .results-table {
            width: 100%;
            border-collapse: collapse;
            margin: 2rem 0;
            background: white;
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
        }

        .results-table th {
            background: var(--bg-dark);
            color: white;
            padding: 1rem;
            text-align: left;
            font-weight: 600;
        }

        .results-table td {
            padding: 1rem;
            border-bottom: 1px solid var(--border-color);
        }

        .results-table tr:last-child td {
            border-bottom: none;
        }

        .highlight-cell {
            font-weight: 700;
            color: var(--primary-blue);
        }

        /* Key Findings Box */
        .key-findings {
            background: #eff6ff;
            border-left: 4px solid var(--accent-cyan);
            padding: 1.5rem;
            margin: 2rem 0;
            border-radius: 4px;
        }

        .key-findings h4 {
            color: var(--accent-cyan);
            font-weight: 700;
            margin-bottom: 1rem;
        }

        .key-findings ul {
            margin-left: 1.5rem;
        }

        .key-findings li {
            margin-bottom: 0.5rem;
            line-height: 1.6;
        }

        /* Citation Box */
        .citation-box {
            background: var(--bg-gray);
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 1.5rem;
            margin: 3rem 0;
        }

        .citation-box h3 {
            font-size: 1rem;
            font-weight: 700;
            margin-bottom: 1rem;
            color: var(--text-dark);
        }

        .citation-text {
            background: white;
            padding: 1rem;
            border-radius: 4px;
            font-family: 'Courier New', monospace;
            font-size: 0.875rem;
            line-height: 1.6;
            margin-bottom: 1rem;
        }

        .citation-btns {
            display: flex;
            gap: 0.5rem;
        }

        .citation-btn {
            padding: 0.5rem 1rem;
            background: white;
            border: 1px solid var(--border-color);
            border-radius: 4px;
            font-size: 0.875rem;
            cursor: pointer;
            transition: background 0.2s;
        }

        .citation-btn:hover {
            background: var(--bg-gray);
        }

        /* Related Content */
        .related-section {
            border-top: 2px solid var(--border-color);
            padding-top: 3rem;
            margin-top: 3rem;
        }

        .related-section h3 {
            font-size: 1.5rem;
            margin-bottom: 1.5rem;
        }

        .related-grid {
            display: grid;
            grid-template-columns: repeat(2, 1fr);
            gap: 1.5rem;
        }

        .related-card {
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 1.5rem;
            transition: all 0.2s;
        }

        .related-card:hover {
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.1);
        }

        .related-tag {
            font-size: 0.75rem;
            color: var(--accent-cyan);
            font-weight: 600;
            text-transform: uppercase;
            margin-bottom: 0.5rem;
        }

        .related-card h4 {
            font-size: 1.125rem;
            margin-bottom: 0.5rem;
        }

        .related-card p {
            font-size: 0.875rem;
            color: var(--text-gray);
            line-height: 1.6;
        }

        /* Sidebar Resources */
        .resource-box {
            background: white;
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 1.5rem;
            margin-bottom: 1.5rem;
        }

        .resource-box h4 {
            font-size: 1rem;
            font-weight: 700;
            margin-bottom: 1rem;
        }

        .resource-link {
            display: flex;
            align-items: center;
            gap: 0.75rem;
            padding: 0.75rem;
            background: var(--bg-gray);
            border-radius: 6px;
            text-decoration: none;
            color: var(--text-dark);
            margin-bottom: 0.75rem;
            transition: background 0.2s;
        }

        .resource-link:hover {
            background: #e5e7eb;
        }

        .resource-icon {
            width: 32px;
            height: 32px;
            background: var(--primary-blue);
            border-radius: 4px;
            display: flex;
            align-items: center;
            justify-content: center;
            color: white;
            font-weight: bold;
            font-size: 0.875rem;
        }

        .resource-text {
            flex: 1;
        }

        .resource-title {
            font-weight: 600;
            font-size: 0.875rem;
            margin-bottom: 0.125rem;
        }

        .resource-meta {
            font-size: 0.75rem;
            color: var(--text-gray);
        }

        /* Metrics Box */
        .metrics-box {
            background: linear-gradient(135deg, var(--primary-blue), #1e40af);
            color: white;
            border-radius: 8px;
            padding: 1.5rem;
        }

        .metrics-box h4 {
            margin-bottom: 1rem;
        }

        .metric-item {
            display: flex;
            justify-content: space-between;
            padding: 0.75rem 0;
            border-bottom: 1px solid rgba(255, 255, 255, 0.2);
        }

        .metric-item:last-child {
            border-bottom: none;
        }

        .metric-value {
            font-size: 1.5rem;
            font-weight: 700;
        }

        @media (max-width: 1024px) {
            .content-wrapper {
                grid-template-columns: 1fr;
            }

            .sidebar {
                position: static;
            }

            .related-grid {
                grid-template-columns: 1fr;
            }
        }
    </style>
</head>
<body>
    <nav>
        <div class="nav-inner">
            <a href="#" class="logo">VFX Intelligence</a>
            <div class="breadcrumb">
                <a href="#">Research</a> / Shot Complexity Classification
            </div>
        </div>
    </nav>

    <article>
        <header class="article-header">
            <span class="paper-tag">Research Paper</span>
            <h1>Multi-Modal Shot Complexity Classification for VFX Production Estimation</h1>
            
            <div class="author-info">
                <div class="author-avatar"></div>
                <div class="author-details">
                    <div class="author-name">Your Name</div>
                    <div class="author-meta">Published November 20, 2025 â€¢ 18 min read</div>
                </div>
            </div>

            <div class="paper-actions">
                <a href="#" class="action-btn btn-primary">
                    ðŸ“¥ Download PDF
                </a>
                <a href="#" class="action-btn btn-secondary">
                    ðŸ’» View Code
                </a>
                <a href="#" class="action-btn btn-secondary">
                    ðŸ“Š Dataset
                </a>
                <a href="#" class="action-btn btn-secondary">
                    ðŸ”— Cite
                </a>
            </div>

            <div class="abstract">
                <h2>Abstract</h2>
                <p>VFX production estimation suffers from 40-60% variance between quoted and actual costs, primarily due to subjective shot complexity assessment. We present a multi-modal classification system combining computer vision (shot frames), natural language processing (shot descriptions), and structured metadata (technical specifications) to predict complexity tier and labor hours with 82% accuracy across 500 production shots from 10 television series. Our system reduces estimation time from 6 hours to 45 minutes per episode while improving accuracy, with potential annual savings of $2-3M per mid-sized studio.</p>
            </div>
        </header>

        <div class="content-wrapper">
            <div class="main-content">
                <h2 id="introduction">1. Introduction</h2>
                <p>Visual effects production operates on a bid-based model where studios estimate labor hours for each shot before work begins. This estimation process combines technical assessment (resolution, frame count, element complexity) with subjective evaluation (artistic difficulty, iterative risk). Industry surveys indicate 40-60% variance between initial estimates and actual production costs, leading to budget overruns and contentious vendor relationships.</p>

                <p>The core challenge is that shot complexity is multidimensional. A shot may be technically simple (clean plate, stationary camera) but artistically complex (subtle performance nuances requiring many iterations). Conversely, technically challenging shots (motion blur, occlusion) may proceed smoothly with experienced artists. Human estimators integrate these factors implicitly through experience, but this expertise doesn't scale and varies significantly between individuals.</p>

                <div class="key-findings">
                    <h4>Key Contributions</h4>
                    <ul>
                        <li>First large-scale dataset of labeled VFX shots with production outcomes (n=500)</li>
                        <li>Multi-modal architecture combining vision, text, and structured features</li>
                        <li>82% classification accuracy and 7.2-hour MAE for regression</li>
                        <li>Production-validated tool deployed at 2 studios</li>
                    </ul>
                </div>

                <h2 id="methodology">2. Methodology</h2>
                
                <h3>2.1 Dataset Construction</h3>
                <p>We collected 500 shots across 10 episodic television series, ranging from subtle compositing (wire removal, beauty work) to heavy CG integration (digital environments, creature work). Each shot includes:</p>

                <div class="figure">
                    <div class="figure-image">Dataset Composition Visualization</div>
                    <div class="figure-caption">
                        <strong>Figure 1:</strong> Distribution of shots across complexity tiers and shot types. The dataset includes 120 "simple comp" shots (clean edges, static camera), 180 "moderate VFX" (motion tracking, paint work), 140 "heavy integration" (CG elements with occlusion), 45 "full CG environments," and 15 "hero creature" shots requiring advanced simulation.
                    </div>
                </div>

                <h3>2.2 Model Architecture</h3>
                <p>Our system employs a three-branch architecture that processes visual, textual, and metadata inputs independently before fusion:</p>

                <div class="code-block">
                    <div class="code-header">
                        <span class="code-lang">Python</span>
                        <button class="copy-btn">Copy</button>
                    </div>
<pre>import torch
import torch.nn as nn
from torchvision import models

class MultiModalComplexityClassifier(nn.Module):
    def __init__(self, num_classes=5):
        super().__init__()
        
        # Vision branch - ResNet50 pretrained on ImageNet
        self.vision_backbone = models.resnet50(pretrained=True)
        self.vision_fc = nn.Linear(2048, 512)
        
        # Text branch - BERT for shot descriptions
        self.text_encoder = BERTEncoder(output_dim=512)
        
        # Metadata branch - structured features
        self.metadata_fc = nn.Sequential(
            nn.Linear(15, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, 512)
        )
        
        # Fusion and classification
        self.fusion = nn.Sequential(
            nn.Linear(1536, 768),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(768, num_classes)
        )
        
    def forward(self, image, text, metadata):
        v_feat = self.vision_fc(self.vision_backbone(image))
        t_feat = self.text_encoder(text)
        m_feat = self.metadata_fc(metadata)
        
        combined = torch.cat([v_feat, t_feat, m_feat], dim=1)
        return self.fusion(combined)</pre>
                </div>

                <h2 id="results">3. Results</h2>
                
                <h3>3.1 Classification Performance</h3>
                <p>We evaluated our model on a held-out test set of 100 shots (20% of dataset) across all complexity tiers:</p>

                <table class="results-table">
                    <thead>
                        <tr>
                            <th>Complexity Tier</th>
                            <th>Precision</th>
                            <th>Recall</th>
                            <th>F1 Score</th>
                            <th>n</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Simple Comp</td>
                            <td>0.89</td>
                            <td>0.91</td>
                            <td>0.90</td>
                            <td>24</td>
                        </tr>
                        <tr>
                            <td>Moderate VFX</td>
                            <td>0.78</td>
                            <td>0.83</td>
                            <td>0.80</td>
                            <td>36</td>
                        </tr>
                        <tr>
                            <td>Heavy Integration</td>
                            <td>0.81</td>
                            <td>0.75</td>
                            <td>0.78</td>
                            <td>28</td>
                        </tr>
                        <tr>
                            <td>Full CG Environment</td>
                            <td>0.88</td>
                            <td>0.80</td>
                            <td>0.84</td>
                            <td>9</td>
                        </tr>
                        <tr>
                            <td>Hero Creature</td>
                            <td>1.00</td>
                            <td>0.67</td>
                            <td>0.80</td>
                            <td>3</td>
                        </tr>
                        <tr>
                            <td><strong>Overall</strong></td>
                            <td class="highlight-cell">0.83</td>
                            <td class="highlight-cell">0.81</td>
                            <td class="highlight-cell">0.82</td>
                            <td>100</td>
                        </tr>
                    </tbody>
                </table>

                <div class="figure">
                    <div class="figure-image">Confusion Matrix Heatmap</div>
                    <div class="figure-caption">
                        <strong>Figure 2:</strong> Confusion matrix showing model predictions vs ground truth. Most errors occur between adjacent complexity tiers (e.g., "Moderate VFX" misclassified as "Heavy Integration"), reflecting genuine ambiguity in these boundary cases.
                    </div>
                </div>

                <h3>3.2 Labor Hour Prediction</h3>
                <p>Beyond classification, we trained a regression head to predict actual labor hours. Mean Absolute Error (MAE) of 7.2 hours represents a 65% improvement over human baseline (20.5 hours MAE).</p>

                <h2 id="discussion">4. Discussion</h2>
                
                <h3>4.1 Production Deployment</h3>
                <p>We deployed our system at two mid-sized VFX studios for a 3-month pilot. VFX supervisors reported:</p>
                
                <ul>
                    <li><strong>Time savings:</strong> Estimation time reduced from 6 hours to 45 minutes per 10-episode season</li>
                    <li><strong>Improved accuracy:</strong> Variance between estimate and actual dropped from 42% to 18%</li>
                    <li><strong>Better resource allocation:</strong> Advance knowledge of shot distribution enabled proactive staffing</li>
                    <li><strong>Vendor negotiation:</strong> Data-driven estimates strengthened bid discussions</li>
                </ul>

                <h3>4.2 Limitations</h3>
                <p>Our model has several important limitations that restrict generalization:</p>

                <ul>
                    <li><strong>Training domain:</strong> Dataset limited to episodic TV; feature films may require retraining</li>
                    <li><strong>Studio-specific factors:</strong> Artist skill levels and tool pipelines vary between studios</li>
                    <li><strong>Creative iteration:</strong> Model cannot predict client-driven revision cycles</li>
                    <li><strong>Novel techniques:</strong> Emerging VFX methods (real-time engines, neural rendering) not represented</li>
                </ul>

                <h2 id="conclusion">5. Conclusion</h2>
                <p>We demonstrate that multi-modal machine learning can substantially improve VFX shot complexity estimation, with 82% classification accuracy and meaningful production benefits. Our approach reduces estimation time by 88% while improving accuracy 65% compared to manual baselines. Beyond immediate cost savings, systematic estimation data creates opportunities for pipeline optimization, capacity planning, and strategic vendor partnerships.</p>

                <p>Future work will extend this approach to feature film production, incorporate temporal models for sequence-level estimation, and develop active learning systems that improve with production feedback. We release our dataset and code publicly to accelerate research in this domain.</p>

                <div class="citation-box">
                    <h3>Cite This Research</h3>
                    <div class="citation-text">
@article{yourname2025complexity,<br>
  title={Multi-Modal Shot Complexity Classification for VFX Production Estimation},<br>
  author={Your Name},<br>
  journal={VFX Intelligence Research},<br>
  year={2025},<br>
  url={https://vfxintelligence.com/research/shot-complexity}<br>
}
                    </div>
                    <div class="citation-btns">
                        <button class="citation-btn">BibTeX</button>
                        <button class="citation-btn">RIS</button>
                        <button class="citation-btn">Plain Text</button>
                    </div>
                </div>

                <div class="related-section">
                    <h3>Related Research</h3>
                    <div class="related-grid">
                        <a href="#" class="related-card">
                            <div class="related-tag">Research</div>
                            <h4>Automated Quality Assessment for Green Screen Plates</h4>
                            <p>Deep learning pipeline for detecting unusable VFX plates before artist assignment.</p>
                        </a>
                        <a href="#" class="related-card">
                            <div class="related-tag">Analysis</div>
                            <h4>The State of AI in VFX Estimation</h4>
                            <p>Industry survey of ML adoption in production planning and cost forecasting.</p>
                        </a>
                    </div>
                </div>
            </div>

            <aside class="sidebar">
                <div class="toc">
                    <h3>Contents</h3>
                    <ul>
                        <li><a href="#introduction">1. Introduction</a></li>
                        <li><a href="#methodology">2. Methodology</a></li>
                        <li><a href="#results">3. Results</a></li>
                        <li><a href="#discussion">4. Discussion</a></li>
                        <li><a href="#conclusion">5. Conclusion</a></li>
                    </ul>
                </div>

                <div class="resource-box">
                    <h4>Resources</h4>
                    <a href="#" class="resource-link">
                        <div class="resource-icon">ðŸ“Š</div>
                        <div class="resource-text">
                            <div class="resource-title">Dataset</div>
                            <div class="resource-meta">500 labeled shots â€¢ 2.3 GB</div>
                        </div>
                    </a>
                    <a href="#" class="resource-link">
                        <div class="resource-icon">ðŸ’»</div>
                        <div class="resource-text">
                            <div class="resource-title">Code Repository</div>
                            <div class="resource-meta">PyTorch â€¢ MIT License</div>
                        </div>
                    </a>
                    <a href="#" class="resource-link">
                        <div class="resource-icon">ðŸ““</div>
                        <div class="resource-text">
                            <div class="resource-title">Jupyter Notebook</div>
                            <div class="resource-meta">Full walkthrough</div>
                        </div>
                    </a>
                    <a href="#" class="resource-link">
                        <div class="resource-icon">ðŸŽ¥</div>
                        <div class="resource-text">
                            <div class="resource-title">Video Presentation</div>
                            <div class="resource-meta">25 min â€¢ Conference talk</div>
                        </div>
                    </a>
                </div>

                <div class="metrics-box">
                    <h4>Key Metrics</h4>
                    <div class="metric-item">
                        <span>Accuracy</span>
                        <span class="metric-value">82%</span>
                    </div>
                    <div class="metric-item">
                        <span>Dataset Size</span>
                        <span class="metric-value">500</span>
                    </div>
                    <div class="metric-item">
                        <span>MAE (hours)</span>
                        <span class="metric-value">7.2</span>
                    </div>
                    <div class="metric-item">
                        <span>Time Saved</span>
                        <span class="metric-value">88%</span>
                    </div>
                </div>
            </aside>
        </div>
    </article>
</body>
</html>
